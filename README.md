# UAV Swarm MARL for Coverage + SLAM

**Multi-Agent Reinforcement Learning** pour couverture 3D urbaine avec **SLAM multi-robots** d√©centralis√©.

![Architecture](docs/assets/architecture_diagram.png)

---

## üéØ Objectif

Entra√Æner un essaim de drones autonomes pour :
- **Coverage 2D** : Maximiser l'exploration d'une zone urbaine
- **SLAM 3D** : Reconstruire une carte 3D collaborative
- **√ânergie** : Gestion autonome de batterie avec retour √† la base
- **Safety** : Contraintes (NFZ, altitude, collisions) via CBF/Lagrangian

**Architecture Hybride** :
- **RL (2D)** : Action space 3D (vx, vy, yaw) pour coverage optimal
- **SLAM (3D)** : Reconstruction pointcloud avec loop closures
- **Altitude** : Contr√¥leur PID automatique (Z_optimal = 7.5m)

---

## üöÄ Quick Start

### 1. Installation (Docker)

```bash
# Clone le repo
git clone https://github.com/princeGedeon/Gazebo_ros_vnc_sim_for_MARL_UAV.git
cd Gazebo_ros_vnc_sim_for_MARL_UAV

# Build Docker (avec GPU NVIDIA)
docker build -t uav_swarm_rl .

# Run container
docker run -it --gpus all \
    -v /tmp/.X11-unix:/tmp/.X11-unix \
    -e DISPLAY=$DISPLAY \
    --name uav_sim \
    uav_swarm_rl
```

### 2. Build ROS2 Workspace

```bash
# Dans le container
cd ~/ros2_ws
colcon build --symlink-install
source install/setup.bash
```

### 3. Lancer Tout (Auto)
# üöÅ ROS 2 Swarm Simulation (Jazzy + Gazebo Harmonic)

**Multi-Agent Reinforcement Learning (MARL)** environment for UAV Swarm Coverage.
Built with **ROS 2 Jazzy**, **Gazebo Harmonic**, **Ray/RLlib**, and **PettingZoo**.

## üöÄ Quick Start (In 1 Command)

**Launch Simulation + Training + Visualization:**
```bash
./scripts/autolaunch_full.sh case_1
```
*(This starts Gazebo, spawns 3 drones, launches RViz, and starts PPO training)*

## üìä Dashboard & Visualization

*   **Ray Dashboard (Training Metrics):** [http://localhost:8265](http://localhost:8265)
*   **NoVNC (Gazebo GUI):** [http://localhost:6080](http://localhost:6080)
*   **RViz:** Launched automatically (Dynamic configuration based on drone count).

---

## üìö Documentation

- **[docs/index.md](docs/index.md)** - Index navigation compl√®te
- **[docs/TECHNICAL_DOCUMENTATION.md](docs/TECHNICAL_DOCUMENTATION.md)** - Architecture d√©taill√©e
- **[docs/WHY_2D_NOT_4D.md](docs/WHY_2D_NOT_4D.md)** - Justification architecture (d√©fense)
- **[docs/GNN_COMMUNICATION.md](docs/GNN_COMMUNICATION.md)** - MAGNET GNN pour communication
- **[docs/TRAINING_SCRIPTS_GUIDE.md](docs/TRAINING_SCRIPTS_GUIDE.md)** - Guide scripts training
- **[docs/RVIZ_2D_GRID_SETUP.md](docs/RVIZ_2D_GRID_SETUP.md)** - Config RViz

---

## üß™ Test & Validation

### Debug Environment

```bash
cd ~/ros2_ws/src/swarm_sim_pkg/swarm_sim/training
python3 debug_rl_env.py
```

**Attendu** : `5/5 tests passed ‚úÖ`

### Baseline Al√©atoire

```bash
python3 evaluate_policy.py --mode random --episodes 50
```

### √âvaluation Trained

```bash
python3 evaluate_policy.py \
    --mode trained \
    --checkpoint outputs/case_1/checkpoint_500000 \
    --episodes 50
```

### Comparaison

```bash
python3 evaluate_policy.py --mode both --checkpoint outputs/case_1/checkpoint_500000
```

**Output** : Graphiques dans `outputs/eval/comparison_*.png`

---

## üéÆ Training Scenarios

### Case 1: MAPPO Simple

```bash
./scripts/autolaunch_full.sh case_1
```

**Caract√©ristiques** :
- Multi-Agent PPO (MAPPO)
- Reward shaping (coverage + energy)
- 500k timesteps (~3h)

### Case 2: MAPPO + Lagrangian

```bash
./scripts/autolaunch_full.sh case_2
```

**Caract√©ristiques** :
- Contraintes soft (NFZ, altitude)
- Lagrange multipliers adaptatifs
- P√©nalit√©s Œª=0.1

### Case 3: MAPPO + CBF

```bash
./scripts/autolaunch_full.sh case_3
```

**Caract√©ristiques** :
- Control Barrier Functions (hard constraints)
- Safety garantie
- Intervention automatique si violation

---

## üìä Architecture

### Environment (PettingZoo)

```python
from swarm_sim.envs.multi_agent.swarm_coverage_env import SwarmCoverageEnv

env = SwarmCoverageEnv(
    num_drones=3,
    max_steps=1000,
    min_height=2.0,
    max_height=12.0,
    nfz_config='default'
)

# Action: [vx, vy, yaw] (3D)
# Observation: [state(14) + lidar(11) + map_local(11√ó11) + neighbors(3√ó2)] = 147D
```

### Reward System

```python
r_total = (
    r_coverage_global * 0.3 +      # Objectif global
    r_coverage_incremental * 0.7 + # Exploration locale
    r_energy +                     # Gestion batterie
    r_collision +                  # P√©nalit√© collision
    r_nfz +                        # P√©nalit√© NFZ
    r_altitude +                   # Respect bounds
    r_proximity                    # √âviter autres drones
)
```

### Altitude Controller (PID)

```python
# Z_optimal calcul√© 1 fois √† l'init (sensor_range / 2 = 7.5m)
vz = PID(target=Z_optimal, current=Z_current)
# kp=1.5, kd=0.3

# Cas sp√©cial: descente √† Z=0.5m √† la station
if distance_to_station < 2.5m:
    vz = PID(target=0.5, current=Z_current)
```

### GNN Communication (MAGNET)

```python
from swarm_sim.models.gnn_communication import MAGNETEncoder

gnn = MAGNETEncoder(obs_dim=147, hidden_dim=128, comm_range=20.0)
enhanced_obs = gnn(observations, positions)  # Message passing
```

**Architecture** :
- Graph Attention Network (GAT) 3 layers
- Communication range : 20m
- PyTorch implementation

---

## üó∫Ô∏è Maps & Outputs

### SLAM 3D

**Output** : `outputs/map_episode_X.pcd`  
**Format** : PointCloud (.pcd)  
**Syst√®me** : `mrg_slam` (multi-robot graph SLAM)

### Coverage 2D

**Output** : `outputs/coverage_2d_episode_X.npy`  
**Format** : NumPy array (sparse grid)  
**R√©solution** : 0.5m

### Visualization

**RViz Topics** :
- `/coverage/global_map` (OccupancyGrid)
- `/coverage/uav_X` (GridCells, couleurs par UAV)
- `/slam/pointcloud` (PointCloud2 3D)

---

## üîß Configuration

### Environment Params

| Param√®tre | Valeur | Description |
|-----------|--------|-------------|
| `num_drones` | 3 | Nombre UAVs |
| `max_steps` | 1000 | Steps par √©pisode |
| `min_height` | 2.0m | Altitude min |
| `max_height` | 12.0m | Altitude max |
| `Z_optimal` | 7.5m | Altitude cruise (auto) |
| `nfz_config` | 'default' | Configuration NFZ |
| `comm_range` | 20.0m | Range GNN |

### Training Params

| Param√®tre | Valeur | Description |
|-----------|--------|-------------|
| `total_timesteps` | 500k | Total steps |
| `checkpoint_freq` | 50k | Fr√©quence save |
| `lr` | 3e-4 | Learning rate |
| `gamma` | 0.99 | Discount factor |

---

## üìà R√©sultats Attendus

| M√©trique | Random | Trained (500k) | Am√©lioration |
|----------|--------|----------------|--------------|
| Episode Return | -100 | +120 | **+220** |
| Coverage (%) | 15% | 65% | **+333%** |
| Battery Efficiency | 0.3 | 0.8 | **+167%** |
| Convergence (steps) | - | 500k | **4√ó vs 4D** |

---

## üõ†Ô∏è Troubleshooting

### Gazebo ne lance pas
```bash
# V√©rifier GPU
nvidia-smi

# Relancer
pkill -f gazebo
ros2 launch swarm_sim super_simulation.launch.py
```

### Topics ROS2 manquants
```bash
ros2 topic list | grep -E "(odom|lidar|coverage)"
```

### Environment test √©choue
```bash
python3 debug_rl_env.py
# Si fails ‚Üí voir traceback
```

### Training tr√®s lent
‚Üí V√©rifier `city_train.sdf` avec `real_time_factor=0.0`

---

## üì¶ Structure Projet

```
gazebo_ros2_vnc/
‚îú‚îÄ‚îÄ README.md                    # Ce fichier
‚îú‚îÄ‚îÄ docs/                        # Documentation
‚îÇ   ‚îú‚îÄ‚îÄ index.md
‚îÇ   ‚îú‚îÄ‚îÄ TECHNICAL_DOCUMENTATION.md
‚îÇ   ‚îú‚îÄ‚îÄ WHY_2D_NOT_4D.md
‚îÇ   ‚îú‚îÄ‚îÄ GNN_COMMUNICATION.md
‚îÇ   ‚îî‚îÄ‚îÄ TRAINING_SCRIPTS_GUIDE.md
‚îú‚îÄ‚îÄ scripts/                     # Automation
‚îÇ   ‚îî‚îÄ‚îÄ autolaunch_full.sh       # Lance tout
‚îú‚îÄ‚îÄ src/swarm_sim_pkg/
‚îÇ   ‚îî‚îÄ‚îÄ swarm_sim/
‚îÇ       ‚îú‚îÄ‚îÄ envs/                # RL environment
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ multi_agent/
‚îÇ       ‚îÇ       ‚îî‚îÄ‚îÄ swarm_coverage_env.py  # PettingZoo env
‚îÇ       ‚îú‚îÄ‚îÄ models/              # GNN, policies
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ gnn_communication.py
‚îÇ       ‚îú‚îÄ‚îÄ training/            # Scripts training
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ debug_rl_env.py         # Debug complet
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ evaluate_policy.py      # Evaluation
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ train_mappo.py
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ train_mappo_lagrangian.py
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ train_mappo_cbf.py
‚îÇ       ‚îî‚îÄ‚îÄ common/              # Utils
‚îÇ           ‚îú‚îÄ‚îÄ occupancy_grid_2d.py    # Coverage 2D
‚îÇ           ‚îî‚îÄ‚îÄ grid_viz_2d.py          # RViz publisher
‚îî‚îÄ‚îÄ rviz_configs/
    ‚îî‚îÄ‚îÄ full_system.rviz         # Config RViz
```

---

## üéì Pour la D√©fense

**Documents cl√©s** :
1. [WHY_2D_NOT_4D.md](docs/WHY_2D_NOT_4D.md) - Justification architecture
2. [TECHNICAL_DOCUMENTATION.md](docs/TECHNICAL_DOCUMENTATION.md) - Probl√®mes r√©solus
3. Graphs comparaison (`outputs/eval/comparison_*.png`)

**D√©mo** :
```bash
# 1. Validation environnement
python3 debug_rl_env.py  # 5/5 tests

# 2. Baseline
python3 evaluate_policy.py --mode random --episodes 50

# 3. Trained
python3 evaluate_policy.py --mode trained --checkpoint outputs/case_1/checkpoint_500000

# 4. Visualisation RViz
rviz2 -d rviz_configs/full_system.rviz
```

---

## üìù Citation

```bibtex
@mastersthesis{gedeon2026marl,
  author = {Prince Gedeon},
  title = {Multi-Agent Reinforcement Learning for UAV Swarm Coverage with 3D SLAM},
  school = {Master IAIA},
  year = {2026}
}
```

---

## üìß Contact

**Auteur** : Prince Gedeon  
**Email** : pgguedje@example.com  
**GitHub** : [princeGedeon](https://github.com/princeGedeon)

---

**Derni√®re mise √† jour** : 2026-01-30  
**Statut** : ‚úÖ Production Ready
